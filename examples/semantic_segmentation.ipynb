{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic_segmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakewakame/easyaiortc/blob/main/examples/semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T5DjQglB0LB"
      },
      "source": [
        "# セマンティックセグメンテーションのデモ\n",
        "セマンティックセグメンテーションの学習済みモデルを動かしてみるサンプルプログラムです。  \n",
        "TensorFlow用、TensorFlowLite用、PyTorch用の3つを用意しました。  \n",
        "  \n",
        "動作させる際には処理速度向上のため、 `ランタイム` > `ランタイムのタイプを変更` から `ハードウェア アクセラレータ` を `GPU` に変更することをお勧めします。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhdYoepqB9Ff"
      },
      "source": [
        "## 1. easyaiortcのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn9huHWTB9SE"
      },
      "source": [
        "!apt install libavdevice-dev libavfilter-dev libopus-dev libvpx-dev pkg-config\n",
        "!pip install git+https://github.com/wakewakame/easyaiortc.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L95-TldLB0Sj"
      },
      "source": [
        "## 2. セマンティックセグメンテーションを行う抽象クラスの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-r5uLyAB94l"
      },
      "source": [
        "from abc import ABC, ABCMeta, abstractmethod\n",
        "\n",
        "class SemanticSegmentation(metaclass = ABCMeta):\n",
        "    @abstractmethod\n",
        "    def estimate(self, input_image):\n",
        "        pass\n",
        "\n",
        "    def colorful(self, estimated_image, original_image=None, alpha=0.5):\n",
        "        estimated_image = cv2.resize(estimated_image, dsize=(original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "        output_image = estimated_image * int(255 / 21)\n",
        "        output_image = cv2.applyColorMap(output_image, cv2.COLORMAP_HSV)\n",
        "        if original_image is not None:\n",
        "            output_image = cv2.addWeighted(\n",
        "                src1=original_image, alpha=alpha,\n",
        "                src2=output_image, beta=1.0-alpha,\n",
        "                gamma=0.0\n",
        "            )\n",
        "        return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpYFHbWXCG1t"
      },
      "source": [
        "## 3. TensorFlow版の実装\n",
        "参考元 : [https://github.com/tensorflow/models/tree/master/research/deeplab](https://github.com/tensorflow/models/tree/master/research/deeplab)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIMPZfuxCHIq"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class TensorFlowSegm(SemanticSegmentation):\n",
        "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "    INPUT_SIZE = 513\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, download_path=None):\n",
        "        url = 'http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz'\n",
        "        #url = 'http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz'\n",
        "        #url = 'http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz'\n",
        "        #url = 'http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz'\n",
        "        if download_path is None:\n",
        "            download_path = os.path.join(os.getcwd(), url.split(\"/\")[-1])\n",
        "        if not os.path.isfile(download_path):\n",
        "            data = urllib.request.urlopen(url).read()\n",
        "            with open(download_path, mode=\"wb\") as f:\n",
        "                f.write(data)\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "        with tarfile.open(download_path) as tar_file:\n",
        "            for tar_info in tar_file.getmembers():\n",
        "                if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                    file_handle = tar_file.extractfile(tar_info)\n",
        "                    graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
        "                    break\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
        "\n",
        "    def estimate(self, input_image):\n",
        "        height, width, _ = input_image.shape\n",
        "        resize_ratio = self.INPUT_SIZE / max(width, height)\n",
        "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "        input_image = cv2.resize(input_image, dsize=target_size, interpolation=cv2.INTER_LINEAR)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            self.OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={self.INPUT_TENSOR_NAME: [input_image]}\n",
        "        )\n",
        "        output_image = batch_seg_map[0].astype(np.uint8)\n",
        "        return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3p4suXdaZjf"
      },
      "source": [
        "## 4. TensorFlowLite版の実装\n",
        "参考元1 : [https://www.tensorflow.org/lite/guide/python](https://www.tensorflow.org/lite/guide/python)  \n",
        "参考元2 : [https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/default/1](https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/default/1)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTj2MfVaaZ4_"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class TensorFlowLiteSegm(SemanticSegmentation):\n",
        "    def __init__(self, download_path=None):\n",
        "        url = 'https://storage.googleapis.com/tfhub-lite-models/tensorflow/lite-model/deeplabv3/1/default/1.tflite'\n",
        "        #url = 'https://storage.googleapis.com/tfhub-lite-models/tensorflow/lite-model/deeplabv3/1/metadata/2.tflite'\n",
        "        if download_path is None:\n",
        "            download_path = os.path.join(os.getcwd(), url.split(\"/\")[-1])\n",
        "        if not os.path.isfile(download_path):\n",
        "            data = urllib.request.urlopen(url).read()\n",
        "            with open(download_path, mode=\"wb\") as f:\n",
        "                f.write(data)\n",
        "        self.interpreter = tf.lite.Interpreter(model_path=download_path, num_threads=None)\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "        self.height, self.width = self.input_details[0]['shape'].tolist()[1:3]\n",
        "        self.output_index = self.output_details[0]['index']\n",
        "\n",
        "    def estimate(self, input_image):\n",
        "        input_image = cv2.resize(input_image, dsize=(self.width, self.height), interpolation=cv2.INTER_LINEAR)\n",
        "        input_data = np.expand_dims(input_image, axis=0)\n",
        "        if self.input_details[0]['dtype'] == np.float32:\n",
        "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
        "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
        "        self.interpreter.invoke()\n",
        "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
        "        output_data = np.squeeze(output_data)\n",
        "        output_image = output_data.argmax(2).astype(np.uint8)\n",
        "        return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTsO10O5CHhu"
      },
      "source": [
        "## 5. PyTorch版の実装\n",
        "参考元1 : [https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)  \n",
        "参考元2 : [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbQfkn7hCH0_"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class PyTorchSegm(SemanticSegmentation):\n",
        "    def __init__(self):\n",
        "        #self.model = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n",
        "        #self.model = torchvision.models.segmentation.fcn_resnet101(pretrained=True)\n",
        "        #self.model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
        "        #self.model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "        self.model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
        "        #self.model = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)\n",
        "        self.model.eval()\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.to('cuda')\n",
        "        self.preprocess = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def estimate(self, input_image):\n",
        "        input_tensor = self.preprocess(input_image)\n",
        "        input_batch = input_tensor.unsqueeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "            input_batch = input_batch.to('cuda')\n",
        "        with torch.no_grad():\n",
        "            output_tensor = self.model(input_batch)['out'][0]\n",
        "        output_image = output_tensor.argmax(0).byte().cpu().numpy()\n",
        "        return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DBBS6RpeWc0"
      },
      "source": [
        "## 6. 画像での実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFLHNsOoedgT"
      },
      "source": [
        "import easyaiortc\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "segm_type = \"TensorFlow\" #@param [\"TensorFlow\", \"TensorFlowLite\", \"PyTorch\"]\n",
        "img_url = \"https://github.com/wakewakame/openpose_ext/blob/main/media/human.jpg?raw=true\" #@param {type:\"string\"}\n",
        "\n",
        "segm = None\n",
        "if segm_type == \"TensorFlow\":\n",
        "    segm = TensorFlowSegm()\n",
        "elif segm_type == \"TensorFlowLite\":\n",
        "    segm = TensorFlowLiteSegm()\n",
        "else:\n",
        "    segm = PyTorchSegm()\n",
        "\n",
        "jpeg = urllib.request.urlopen(img_url).read()\n",
        "img = cv2.imdecode(np.asarray(bytearray(jpeg), dtype=\"uint8\"), cv2.IMREAD_COLOR)\n",
        "\n",
        "segm_img = img\n",
        "segm_img = cv2.cvtColor(segm_img, cv2.COLOR_BGR2RGB)\n",
        "segm_img = segm.estimate(segm_img)\n",
        "segm_img = segm.colorful(segm_img, img)\n",
        "\n",
        "cv2_imshow(segm_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tMAGU6uCIXS"
      },
      "source": [
        "## 7. AppRTCでの実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Ow7UdeCIkX"
      },
      "source": [
        "import easyaiortc\n",
        "\n",
        "segm_type = \"TensorFlow\" #@param [\"TensorFlow\", \"TensorFlowLite\", \"PyTorch\"]\n",
        "segm = None\n",
        "if segm_type == \"TensorFlow\":\n",
        "    segm = TensorFlowSegm()\n",
        "elif segm_type == \"TensorFlowLite\":\n",
        "    segm = TensorFlowLiteSegm()\n",
        "else:\n",
        "    segm = PyTorchSegm()\n",
        "\n",
        "# 接続の開始\n",
        "rtc = easyaiortc.EasyAppRTC(preview=True, width=1280, height=720)\n",
        "\n",
        "try:\n",
        "    # 接続されている間はループ\n",
        "    while rtc.is_alive():\n",
        "        # 映像の受信\n",
        "        img = rtc.get()\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        segm_img = img\n",
        "        segm_img = cv2.cvtColor(segm_img, cv2.COLOR_BGR2RGB)\n",
        "        segm_img = segm.estimate(segm_img)\n",
        "        segm_img = segm.colorful(segm_img, img)\n",
        "\n",
        "        # 回転した映像を送信\n",
        "        rtc.put(segm_img)\n",
        "\n",
        "# Ctrl+Zで終了\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
